{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c06e9d8-9115-450d-a07f-09aac849ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c95ca97d-96ef-4aa4-bba1-bfd6a1022397",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain2prompt = {\n",
    "    \"city\": \"{} lives in the city of\",\n",
    "    \"country\": \"{} lives in the country of\",\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9ca217a-066f-42ea-a22c-9534b277ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_full_token(x):\n",
    "    return tokenizer.convert_tokens_to_string([tokenizer.tokenize(x)[0]]).strip() == x.strip().split()[0]\n",
    "\n",
    "def correlation(Y1_train, Y2_train, Y1_test, Y2_test):\n",
    "\n",
    "    weights = np.zeros((Y1_train.shape[-1], Y2_train.shape[-1]))\n",
    "    bias = np.zeros((1, Y2_train.shape[-1]))\n",
    "    \n",
    "    for i in tqdm(range(Y2_train.shape[1])):\n",
    "        model = LinearRegression()\n",
    "        model.fit(Y1_train, Y2_train[:, i])\n",
    "        weights[:, i] = model.coef_ \n",
    "        bias[0, i] = model.intercept_ \n",
    "    \n",
    "    Y2_pred = Y1_test @ weights + bias\n",
    "\n",
    "    metrics = [pearsonr(Y2_pred[:, idx], Y2_test[:, idx]).statistic for idx in range(len(Y2_names))]\n",
    "    \n",
    "    avg, std = np.mean(metrics), np.std(metrics)\n",
    "\n",
    "    return metrics, avg, std, weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab9c362b-cf15-4249-afbd-9b0c4e4cf71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logit(prompt, generator):\n",
    "    items = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    logit = generator(**items).logits[0, -1]\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "061375bb-409f-4d11-90c3-b3935355e980",
   "metadata": {},
   "outputs": [],
   "source": [
    "genid2model_path = {\n",
    "    \"8b-pt\":\"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"8b-it\":\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "660fd9fc-c03e-411d-a798-08762df67fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19cd8e10-dcc8-4ab2-99cf-4d8abc7e9729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70272a67a56b47c99a02f2bfdafcb57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ab04469ed5405f874cc0588629e997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1777670f06ee4da883d568a526d9b9bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4277e7f2a2974e5cbfed259bbcc2514a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2319aad9c784880891c8f41b220195a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc4983295494de6afc3be522040e96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b5eae22124b4751a8f80bba7b995ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city country 0.7580538680629731 0.05411872072291359\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model_id = \"8b-pt\"\n",
    "model_sft_id = \"8b-it\"\n",
    "Y1, Y2 = \"city\", \"country\"\n",
    "\n",
    "model_path = genid2model_path[model_id]\n",
    "model_sft_path = genid2model_path[model_id]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "generator = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "generator_sft = AutoModelForCausalLM.from_pretrained(model_sft_path).to(device)\n",
    "names = np.random.choice(list(tokenizer.get_vocab().keys()), 1000)\n",
    "\n",
    "cache = {}\n",
    "\n",
    "Y1_domain = [\" \"+y.strip() for y in json.load(open(f\"{Y1}.json\"))]\n",
    "Y2_domain = [\" \"+y.strip() for y in json.load(open(f\"{Y2}.json\"))]\n",
    "\n",
    "Y1_domain = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(y)[0]) for y in Y1_domain if is_full_token(y)]\n",
    "Y2_domain = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(y)[0]) for y in Y2_domain if is_full_token(y)]\n",
    "\n",
    "Y1_domain = list(set(Y1_domain))\n",
    "Y2_domain = list(set(Y2_domain))\n",
    "\n",
    "with torch.no_grad():\n",
    "    Y1_prompt = domain2prompt[Y1]\n",
    "    Y1_logits = np.stack([get_logit(Y1_prompt.format(name), generator).detach().cpu().numpy() for name in tqdm(names[::2])])\n",
    "    Y1_logits = Y1_logits[:, Y1_domain]\n",
    "    \n",
    "    Y2_prompt = domain2prompt[Y2]\n",
    "    Y2_logits = np.stack([get_logit(Y2_prompt.format(name), generator).detach().cpu().numpy() for name in tqdm(names[::2])])\n",
    "    Y2_logits = Y2_logits[:, Y2_domain]\n",
    "    \n",
    "    Y1_prompt = domain2prompt[Y1]\n",
    "    Y1_sft_logits = np.stack([get_logit(Y1_prompt.format(name), generator_sft).detach().cpu().numpy() for name in tqdm(names[1::2])])\n",
    "    Y1_sft_logits = Y1_sft_logits[:, Y1_domain]\n",
    "    \n",
    "    Y2_prompt = domain2prompt[Y2]\n",
    "    Y2_sft_logits = np.stack([get_logit(Y2_prompt.format(name), generator_sft).detach().cpu().numpy() for name in tqdm(names[1::2])])\n",
    "    Y2_sft_logits = Y2_sft_logits[:, Y2_domain]\n",
    "\n",
    "Y1_names = [tokenizer.decode(y1) for y1 in Y1_domain]\n",
    "Y2_names = [tokenizer.decode(y2) for y2 in Y2_domain]\n",
    "\n",
    "metrics, avg, std, weights, bias = correlation(Y1_logits, Y2_logits, Y1_sft_logits, Y2_sft_logits)\n",
    "print(Y1, Y2, avg, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c4dd72d-711b-40f4-b280-425c674ee27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tokyo [' Trinidad', ' Catalonia', ' Austria', ' Japan', ' Luxembourg']\n"
     ]
    }
   ],
   "source": [
    "Y1_name = \" Tokyo\"\n",
    "idx = Y1_names.index(Y1_name)\n",
    "top_jds = weights[idx].argsort()[::-1][:5]\n",
    "top_Y2_names = [Y2_names[jdx] for jdx in top_jds]\n",
    "print(Y1_name, top_Y2_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d86914b-bdfb-4c0b-81bd-42caafa7636a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
